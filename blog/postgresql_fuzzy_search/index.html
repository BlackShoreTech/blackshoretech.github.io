<!DOCTYPE html><html lang="en"><head><meta charSet="UTF-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/><link rel="apple-touch-icon" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="icon" href="/favicon.ico"/><title>PostgreSQL Fuzzy Text Search: Not so fuzzy to fuzziest</title><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="So you have a bunch of data that comes from some human source (Free text form fields, reviews, blogs, classified ads, social media) and you want to do some analysis on it."/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@brsc2909"/><meta name="twitter:creator" content="@brsc2909"/><meta property="og:title" content="PostgreSQL Fuzzy Text Search: Not so fuzzy to fuzziest"/><meta property="og:description" content="So you have a bunch of data that comes from some human source (Free text form fields, reviews, blogs, classified ads, social media) and you want to do some analysis on it."/><meta property="og:image" content="https://cdn.hashnode.com/res/hashnode/image/upload/v1654533752867/bDBd4SD-S.png?w=1600&amp;h=840&amp;fit=crop&amp;crop=entropy&amp;auto=compress,format&amp;format=webp"/><meta property="og:locale" content="en"/><meta property="og:site_name" content="BlackShore Technology"/><meta name="next-head-count" content="18"/><link rel="preload" href="/_next/static/css/6edd04880f113090.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6edd04880f113090.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-2cd43fd5c66c1905.js" defer=""></script><script src="/_next/static/chunks/framework-5f4595e5518b5600.js" defer=""></script><script src="/_next/static/chunks/main-9e46c965f9e1563d.js" defer=""></script><script src="/_next/static/chunks/pages/_app-7fe1566c856316c8.js" defer=""></script><script src="/_next/static/chunks/275-a1f904950f98e0c2.js" defer=""></script><script src="/_next/static/chunks/441-e7ff11799e4942a2.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-d7ebf928d204b7f6.js" defer=""></script><script src="/_next/static/huS74PAfV3qTdbv0QEjjy/_buildManifest.js" defer=""></script><script src="/_next/static/huS74PAfV3qTdbv0QEjjy/_ssgManifest.js" defer=""></script><style id="__jsx-4057204961">.navbar.jsx-4057204961 li:not(:first-child){margin-top:0;}.navbar.jsx-4057204961 li:not(:last-child){margin-right:1.25rem;}</style></head><body><div id="__next" data-reactroot=""><div><div class="bg-gray-900"><div class="max-w-screen-lg mx-auto px-3 py-3 undefined"><div class="jsx-4057204961 flex flex-wrap justify-between items-center"><div class="jsx-4057204961"><a class="jsx-4057204961" href="/"><span class="text-gray-100 inline-flex items-center font-semibold text-xl"><svg clip-rule="evenodd" fill-rule="evenodd" width="32" height="32" image-rendering="optimizeQuality" shape-rendering="geometricPrecision" text-rendering="geometricPrecision" viewBox="0 0 110.99997 120.00004" version="1.1" id="svg68" xmlns="http://www.w3.org/2000/svg"><defs id="defs58"><linearGradient gradientUnits="userSpaceOnUse" id="a" x1="87.309998" x2="7662.6299" y1="3022.4099" y2="14087.64"><stop offset="0" stop-color="#004790" id="stop53"></stop><stop offset="1" stop-color="#1297e0" id="stop55"></stop></linearGradient></defs><g id="g66" transform="scale(0.008,0.0077)"><path d="M 6348.16,5590.89 8916.54,7073.7 10687.81,6051.09 V 3085.38 L 5343.87,0 3127.55,1279.53 8471.59,4364.91 Z" fill="#0067b0" id="path60"></path><path d="M 7912.25,7653.67 0,3085.38 V 6051.09 L 8536.06,10979.42 5343.87,12822.36 0,9736.98 v 2604.18 l 5343.87,3085.29 5343.94,-3085.29 v -3085.1 z" fill="url(#a)" id="path62"></path><path d="M 8471.59,4364.91 10687.81,3085.38 7791.94,3972.5 Z" fill="#003780" id="path64"></path></g></svg></span></a></div><nav class="jsx-4057204961"><ul class="jsx-4057204961 navbar flex items-center font-medium text-xl text-gray-300"><li><a href="/about/">About us</a></li><li><a>Services</a></li><li><a href="/blog/">Blog</a></li></ul></nav></div></div></div><div class="max-w-screen-lg mx-auto px-3 py-16 prose prose-slate"><div class="mb-12 text-center"><h2 class="text-4xl text-gray-900 font-bold">PostgreSQL Fuzzy Text Search: Not so fuzzy to fuzziest</h2><div class="mt-4 text-xl md:px-20">So you have a bunch of data that comes from some human source (Free text form fields, reviews, blogs, classified ads, social media) and you want to do some analysis on it.</div></div><article><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1654533752867/bDBd4SD-S.png?w=1600&amp;h=840&amp;fit=crop&amp;crop=entropy&amp;auto=compress,format&amp;format=webp" alt="PostgreSQL Fuzzy Text Search: Not so fuzzy to fuzziest"/><div>June 7, 2022</div><div><p>So you have a bunch of data that comes from some human source (Free text form fields, reviews, blogs, classified ads, social media) and you want to do some analysis on it. but with people being the way they are, you&#39;re going to have some problems:</p>
<ol>
<li>A lot of words are commonly miss-spelled (definitely-&gt; definitly etc).</li>
<li>Regional differences. e.g American and British English (color/colour, analyse/analyze)</li>
<li>Creative ways of spelling to add dramatic effect. (heyyy, whaaaaat!, noooo!)</li>
</ol>
<p>All of this, plus more will affect your results and make it difficult to do any accurate analysis of the data (such as grouping similar topics together, etc). Luckily PostgreSQL comes packaged with a number of really useful tools that make life a lot easier for us. This is a complex topic and I&#39;m only going to touch on the basics. But there is enough here to cover most basic and possibly some more complex use-cases.  </p>
<h2 id="simple-pattern-matching-a-little-fuzzy">Simple Pattern matching (A little Fuzzy)</h2>
<h4 id="like">LIKE</h4>
<p>Useful when you have a good idea of what the data and queries look like but it&#39;s difficult to create something generic enough to be useful in a general text dataset. With this, you only have two ways to match the text. <code>%</code> is used as a wildcard for 0 or more characters of any value, and <code>_</code> matches a single character of any value.</p>
<ul>
<li><code>LIKE</code>: use wildcards and character substitution, Case sensitive</li>
</ul>
<pre><code class="language-sql">SELECT &#39;Hello world&#39; LIKE &#39;He__o %&#39;;  -- TRUE
</code></pre>
<ul>
<li><code>ILIKE</code>: use wildcards and character substitution, Case insensitive</li>
</ul>
<pre><code class="language-sql">SELECT &#39;Hello world&#39; ILIKE &#39;h_llo _%&#39;; -- TRUE
</code></pre>
<ul>
<li><code>NOT LIKE</code>/<code>NOT ILIKE</code>: inverse of LIKE or ILIKE</li>
</ul>
<pre><code class="language-sql">SELECT &#39;Hello world&#39; NOT ILIKE &#39;%_llo world%&#39;; -- FALSE
</code></pre>
<h4 id="regex">Regex</h4>
<p>A little more advanced than the &quot;LIKE&quot; operator. With regex, you have a lot more control over the pattern matching. PostgreSQL comes with two standard ways to do this. </p>
<ul>
<li><code>[NOT] SIMILAR TO</code>: Uses a simpler SQL standard expression syntax which is kind of like a mix between the LIKE syntax and POSIX regular expressions. You can prepend <code>NOT</code> to negate the expression.</li>
</ul>
<pre><code class="language-sql">SELECT &#39;Hello world&#39; SIMILAR TO &#39;H(e|a)l+o %&#39;; --TRUE
</code></pre>
<ul>
<li><code>~</code>/<code>!~</code>/<code>~*</code>/<code>!~*</code>: More powerful POSIX syntax that you may already be familiar with in other languages. <code>*</code> makes the expressions case-insensitive. <code>!</code> negates the expression</li>
</ul>
<pre><code class="language-sql">SELECT &#39;Hello world&#39; ~ &#39;^H(e|a)l{1,2}o [a-zA-Z]{5}$&#39;; -- TRUE
</code></pre>
<h3 id="improving-performance">Improving performance</h3>
<p>In certain scenarios it&#39;s possible to speed up your queries using a special operator class for a BTREE index. <code>text_pattern_ops</code> and <code>varchar_pattern_ops</code> allow you to index a text or varchar field. However, this is only effective if your queries are <em>left-anchored</em> (No leading wildcard) e.g <code>WHERE text_fields LIKE &#39;hell_ %&#39;</code></p>
<p>I&#39;ve only covered the basics of what you can do with regex and PostgreSQL, so I suggest looking at the docs below to learn more. </p>
<p><strong>See:</strong> <a href="https://www.postgresql.org/docs/current/functions-matching.html">PostgreSQL Docs -&gt; pattern-matchine</a></p>
<h2 id="text-search-vectors-fuzzyish">Text Search Vectors (Fuzzy(ish))</h2>
<p>This is probably the most efficient option for performing a full-text search. It works by removing all the stop words (it, the, as, by, ...) and duplicates from your text and reducing each word into its main component. For example <em>quick</em>, <em>quickly</em> just becomes <em>quick</em> and <em>product</em>, <em>production</em>, <em>products</em> becomes <em>product</em>. This provides a small bit of fuzziness to results as the query does not need the exact word. One caveat with tsvectors is that to use it effectively you need to know the language of the text. you can use the <code>&#39;simple&#39;</code> config option but you lose a lot of the efficiencies. </p>
<p>The first function you&#39;ll need is <code>to_tsvector(config, text)</code>. The result of this is a special datatype <code>tsvector</code> that contains each component along with its index in the original text.</p>
<pre><code class="language-sql">select to_tsvector(&#39;english&#39;, &#39;the quick brown fox ran quickly to the other foxes&#39;);
               to_tsvector                
-------------------------------------
 &#39;brown&#39;:3 &#39;fox&#39;:4,10 &#39;quick&#39;:2,6 &#39;ran&#39;:5
(1 row)
</code></pre>
<p>The second thing you need is the query generator, which comes in a few different flavors</p>
<ul>
<li><code>to_tsquery(config, text) -&gt; tsquery</code>: Creates a basic query from a single token or multiple if
used with boolean operators.</li>
</ul>
<pre><code class="language-sql">SELECT to_tsquery(&#39;english&#39;, &#39;hello&#39;);  --&gt; &#39;hello&#39;
-- OR
SELECT to_tsquery(&#39;english&#39;, &#39;hello &amp; worlds&#39;);  --&gt; &#39;hello&#39; &amp; &#39;world&#39;
</code></pre>
<ul>
<li><code>plainto_tsquery(config, text) -&gt; tsquery</code>: Accepts a more generic search term. by default each word in the query is an &quot;&amp;&quot; operation</li>
</ul>
<pre><code class="language-sql">SELECT plainto_tsquery(&#39;english&#39;, &#39;hello world&#39;); --&gt; &#39;hello&#39; &amp; &#39;world&#39;
</code></pre>
<ul>
<li><code>websearch_to_tsquery(config, text) -&gt; tsquery</code>: This one is a bit more sophisticated and my favourite. It uses a google type syntax for searching.</li>
</ul>
<pre><code class="language-sql">SELECT websearch_to_tsquery(&#39;simple&#39;, &#39;&quot;hello there&quot; -world&#39;);  --&gt;  &#39;hello&#39; &lt;-&gt; &#39;there&#39; &amp; !&#39;world&#39;
</code></pre>
<h3 id="example-usage">Example usage</h3>
<pre><code class="language-sql">SELECT message FROM mock_data
WHERE 
    to_tsvector(&#39;english&#39;, message) 
    @@ 
    websearch_to_tsquery(&#39;english&#39;, &#39;product killer -content&#39;)
LIMIT 5; 
             message             
---------------------------------
 productize killer architectures
 productize killer synergies
(2 rows)
</code></pre>
<h3 id="improving-performance-1">Improving performance</h3>
<p>To get some really good performance on your queries. Create a generated column with the tsvector data and then add a GIN index to that column. </p>
<pre><code class="language-sql">ALTER TABLE mock_data 
    ADD COLUMN ts_message_col tsvector 
    GENERATED ALWAYS AS (to_tsvector(&#39;english&#39;, message)) 
    STORED;

CREATE INDEX idx_tsvector_message ON mock_data USING GIN(ts_message_col);
</code></pre>
<pre><code class="language-sql">SELECT message FROM mock_data
WHERE 
    ts_message_col @@ websearch_to_tsquery(&#39;english&#39;, &#39;product or content&#39;)
LIMIT 5; 
              message              
-----------------------------------
 productize extensible initiatives
 target value-added content
 productize visionary content
 monetize proactive content
 synthesize cross-media content
(5 rows)
</code></pre>
<h2 id="trigrams-fuzzier">Trigrams (Fuzzier)</h2>
<blockquote>
<p><a href="https://www.postgresql.org/docs/current/pgtrgm.html">pg_trgm</a> module required: <code>CREATE extension pg_trgm;</code></p>
</blockquote>
<p>As the name suggests, a trigram is a series of three consecutive characters from a string. For example, take the string <em>&quot;Hello world&quot;.</em></p>
<pre><code class="language-sql">SELECT show_trgm(&#39;Hello world&#39;);
                           show_trgm                           
---------------------------------------------------------------
 {&quot;  h&quot;,&quot;  w&quot;,&quot; he&quot;,&quot; wo&quot;,ell,hel,&quot;ld &quot;,llo,&quot;lo &quot;,orl,rld,wor}
</code></pre>
<p>In PostgreSQL, trigrams are used to generate a similarity score between two strings. 
<a href="https://www.postgresql.org/docs/current/pgtrgm.html">pg_trgm</a> provides us with three functions for this:</p>
<ul>
<li><code>similarity(string, string)</code>: Similarity between the whole first and second string</li>
</ul>
<pre><code class="language-sql">SELECT similarity(&#39;hello&#39;, &#39;Helo world&#39;); --&gt; 0.30769232
-- OR
SELECT 1 - (&#39;hello&#39; &lt;-&gt; &#39;Helo world&#39;);  --&gt; 0.307692289352417
</code></pre>
<ul>
<li><code>word_similarity(string, string)</code>: The greatest similarity between the first string and any substring of the second string</li>
</ul>
<pre><code class="language-sql">SELECT word_similarity(&#39;hello&#39;, &#39;Helo world&#39;); --&gt; 0.5714286
-- OR
SELECT 1 - (&#39;hello&#39; &lt;&lt;-&gt; &#39;Helo world&#39;); --&gt; 0.5714285969734192
</code></pre>
<ul>
<li><code>strict_word_similarity(string, string)</code>: The greatest similarity between the first string and any whole word in the second string</li>
</ul>
<pre><code class="language-sql">SELECT strict_word_similarity(&#39;hello&#39;, &#39;Helo world&#39;); --&gt; 0.5714286
-- OR
SELECT 1 - (&#39;hello&#39; &lt;&lt;&lt;-&gt; &#39;Helo world&#39;); --&gt; 0.5714285969734192
</code></pre>
<h4 id="or-if-you-want-the-boolean-results">Or if you want the boolean results.</h4>
<pre><code class="language-sql">SELECT (&#39;hello&#39; % &#39;Helo world&#39;); --&gt; similarity TRUE
SELECT (&#39;hello&#39; &lt;% &#39;Helo world&#39;); --&gt; word_similarity FALSE
SELECT (&#39;hello&#39; &lt;&lt;% &#39;Helo world&#39;); --&gt; strict_word_similarity TRUE
</code></pre>
<p>The result of this depends on the following GUC parameters respectively</p>
<ul>
<li><code>pg_trgm.similarity_threshold</code> (default 0.3)</li>
<li><code>pg_trgm.word_similarity_threshold</code> (default 0.6)</li>
<li><code>pg_trgm.strict_word_similarity_threshold</code> (default 0.5)</li>
</ul>
<h3 id="improving-performance-2">Improving performance</h3>
<p>Conveniently pg_trgm module provides GiST and GIN index operator classes that allow you to create an index over a text column. I haven&#39;t tested this out fully but apparently, the GIST index provides better performance. </p>
<pre><code class="language-sql">CREATE INDEX trgm_idx_text_column ON test_table USING GIST (text_column gist_trgm_ops);
-- OR
CREATE INDEX trgm_idx_text_column ON test_table USING GIN (text_column gin_trgm_ops);
</code></pre>
<p><strong>See:</strong> <a href="https://www.postgresql.org/docs/current/pgtrgm.html">PostgreSQL Docs: pg_trgm</a></p>
<h2 id="levenshtein-distance-fuzzier">Levenshtein distance (Fuzzier)</h2>
<blockquote>
<p><a href="https://www.postgresql.org/docs/current/fuzzystrmatch.html">fuzzystrmatch</a> module required: <code>CREATE extension fuzzystrmatch;</code></p>
</blockquote>
<p>Levenshtein distance is a measure of the similarity between two strings, measured in terms of the number of characters that need to be changed in order to turn one string into the other. </p>
<pre><code class="language-sql">SELECT 
    first_name, 
    levenshtein(first_name, &#39;Bobby&#39;) AS difference FROM mock_data
WHERE levenshtein(first_name, &#39;Bobby&#39;) &lt; 3
ORDER BY 2
LIMIT 5;
 first_name | difference 
------------+------------
 Bobby      |          0
 Bobbi      |          1
 Bobbi      |          1
 Bibby      |          1
 Toby       |          2
(5 rows)
</code></pre>
<h3 id="performance-improvements">Performance Improvements</h3>
<p>One of the issues with the Levenshtein method is that there is no way to index it as the index would need to know the input. However, there is something we can do. We can reduce the number of records it has to process by combining it with one of the more fuzzy options below.</p>
<p><strong>See:</strong> <a href="https://www.postgresql.org/docs/current/fuzzystrmatch.html#id-1.11.7.24.7">PostgreSQL Docs: fuzzystrmatch</a></p>
<h2 id="phonetic-similarity-very-fuzzy">Phonetic similarity (Very Fuzzy)</h2>
<blockquote>
<p><a href="https://www.postgresql.org/docs/current/fuzzystrmatch.html">fuzzystrmatch</a> module required: <code>CREATE extension fuzzystrmatch;</code></p>
</blockquote>
<p>For me, I found these next couple of methods particularly interesting. Instead of measuring how similar words are to each other in terms of individual characters. We can actually compare them by how they sound when they are spoken.
fuzzystrmatch provides three functions out of the box for this.</p>
<ul>
<li><code>soundex(string) -&gt; text</code>: converts a string to its Soundex code.</li>
</ul>
<pre><code class="language-sql">SELECT soundex(&#39;Anne&#39;), soundex(&#39;Ann&#39;), difference(&#39;Anne&#39;, &#39;Ann&#39;);
 soundex | soundex | difference 
---------+---------+------------
 A500    | A500    |          4
(1 row)
</code></pre>
<ul>
<li><code>metaphone(string, max_output_length) -&gt; text</code>: like Soundex, is based on the idea of constructing a representative code for an input string.</li>
</ul>
<pre><code class="language-sql">SELECT metaphone(&#39;brendan&#39;, 10), metaphone(&#39;brandon&#39;, 10);
 metaphone | metaphone 
-----------+-----------
 BRNTN     | BRNTN
(1 row)
</code></pre>
<ul>
<li><code>dmetaphone(string) -&gt; text</code>/`dmetaphone_alt(string) -&gt; text: computes two “sounds like” strings for a given input string — a “primary” and an “alternate”. In most cases, they are the same, but for non-English names especially they can be a bit different, depending on pronunciation. These functions compute the primary and alternate codes</li>
</ul>
<pre><code class="language-sql">SELECT dmetaphone_alt(&#39;brendan&#39;), dmetaphone(&#39;Brandon&#39;);
 dmetaphone_alt | dmetaphone 
----------------+------------
 PRNT           | PRNT
(1 row)
</code></pre>
<h3 id="performance-improvements-1">performance improvements</h3>
<p>Each of these methods can be indexed using a normal function based index</p>
<pre><code class="language-sql">CREATE INDEX idx_sdx_first_name ON mock_data (soundex(first_name));
--OR
CREATE INDEX idx_mtf_first_name ON mock_data (metaphone(first_name, 10));
--OR
CREATE INDEX idx_dmtf_first_name ON mock_data (dmetaphone(first_name));
</code></pre>
<p> I mentioned above that we can improve the Levenshtein method by combining it with one of these methods. Once you&#39;ve indexed the column for one of the phonetics functions you can use that to reduce the dataset and use Levenshtein to finish the filtering</p>
<pre><code class="language-sql">SELECT 
    first_name, 
    levenshtein(first_name, &#39;Bobby&#39;) AS difference FROM mock_data
WHERE 
    soundex(first_name) = soundex(&#39;bobby&#39;)
AND 
    levenshtein(first_name, &#39;Bobby&#39;) &lt; 3
ORDER BY 2
LIMIT 5;
 first_name | difference 
------------+------------
 Bobby      |          0
 Bobbi      |          1
 Bobbi      |          1
 Bibby      |          1
 Bobbie     |          2
(5 rows)
</code></pre>
<p><strong>See:</strong> <a href="https://www.postgresql.org/docs/current/fuzzystrmatch.html">PostgreSQL Docs: fuzzystrmatch</a></p>
</div></article></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"PostgreSQL Fuzzy Text Search: Not so fuzzy to fuzziest","date":"June 7, 2022","author":"Brendan","excerpt":"So you have a bunch of data that comes from some human source (Free text form fields, reviews, blogs, classified ads, social media) and you want to do some analysis on it.","cover_image":"https://cdn.hashnode.com/res/hashnode/image/upload/v1654533752867/bDBd4SD-S.png?w=1600\u0026h=840\u0026fit=crop\u0026crop=entropy\u0026auto=compress,format\u0026format=webp"},"slug":"postgresql_fuzzy_search","content":"So you have a bunch of data that comes from some human source (Free text form fields, reviews, blogs, classified ads, social media) and you want to do some analysis on it. but with people being the way they are, you're going to have some problems:\n1. A lot of words are commonly miss-spelled (definitely-\u003e definitly etc).\n2. Regional differences. e.g American and British English (color/colour, analyse/analyze)\n3. Creative ways of spelling to add dramatic effect. (heyyy, whaaaaat!, noooo!)\n\nAll of this, plus more will affect your results and make it difficult to do any accurate analysis of the data (such as grouping similar topics together, etc). Luckily PostgreSQL comes packaged with a number of really useful tools that make life a lot easier for us. This is a complex topic and I'm only going to touch on the basics. But there is enough here to cover most basic and possibly some more complex use-cases.  \n\n## Simple Pattern matching (A little Fuzzy)\n#### LIKE\nUseful when you have a good idea of what the data and queries look like but it's difficult to create something generic enough to be useful in a general text dataset. With this, you only have two ways to match the text. `%` is used as a wildcard for 0 or more characters of any value, and `_` matches a single character of any value.\n\n - `LIKE`: use wildcards and character substitution, Case sensitive\n```sql\nSELECT 'Hello world' LIKE 'He__o %';  -- TRUE\n```\n - `ILIKE`: use wildcards and character substitution, Case insensitive\n```sql\nSELECT 'Hello world' ILIKE 'h_llo _%'; -- TRUE\n```\n - `NOT LIKE`/`NOT ILIKE`: inverse of LIKE or ILIKE\n```sql\nSELECT 'Hello world' NOT ILIKE '%_llo world%'; -- FALSE\n```\n\n#### Regex\nA little more advanced than the \"LIKE\" operator. With regex, you have a lot more control over the pattern matching. PostgreSQL comes with two standard ways to do this. \n - `[NOT] SIMILAR TO`: Uses a simpler SQL standard expression syntax which is kind of like a mix between the LIKE syntax and POSIX regular expressions. You can prepend `NOT` to negate the expression.\n```sql\nSELECT 'Hello world' SIMILAR TO 'H(e|a)l+o %'; --TRUE\n```\n - `~`/`!~`/`~*`/`!~*`: More powerful POSIX syntax that you may already be familiar with in other languages. `*` makes the expressions case-insensitive. `!` negates the expression\n```sql\nSELECT 'Hello world' ~ '^H(e|a)l{1,2}o [a-zA-Z]{5}$'; -- TRUE\n```\n\n### Improving performance\nIn certain scenarios it's possible to speed up your queries using a special operator class for a BTREE index. `text_pattern_ops` and `varchar_pattern_ops` allow you to index a text or varchar field. However, this is only effective if your queries are *left-anchored* (No leading wildcard) e.g `WHERE text_fields LIKE 'hell_ %'`\n \n\nI've only covered the basics of what you can do with regex and PostgreSQL, so I suggest looking at the docs below to learn more. \n\n**See:** [PostgreSQL Docs -\u003e pattern-matchine](https://www.postgresql.org/docs/current/functions-matching.html)\n\n## Text Search Vectors (Fuzzy(ish))\nThis is probably the most efficient option for performing a full-text search. It works by removing all the stop words (it, the, as, by, ...) and duplicates from your text and reducing each word into its main component. For example *quick*, *quickly* just becomes *quick* and *product*, *production*, *products* becomes *product*. This provides a small bit of fuzziness to results as the query does not need the exact word. One caveat with tsvectors is that to use it effectively you need to know the language of the text. you can use the `'simple'` config option but you lose a lot of the efficiencies. \n\nThe first function you'll need is `to_tsvector(config, text)`. The result of this is a special datatype `tsvector` that contains each component along with its index in the original text.\n\n```sql\nselect to_tsvector('english', 'the quick brown fox ran quickly to the other foxes');\n               to_tsvector                \n-------------------------------------\n 'brown':3 'fox':4,10 'quick':2,6 'ran':5\n(1 row)\n```\n\nThe second thing you need is the query generator, which comes in a few different flavors\n - `to_tsquery(config, text) -\u003e tsquery`: Creates a basic query from a single token or multiple if\nused with boolean operators.\n```sql\nSELECT to_tsquery('english', 'hello');  --\u003e 'hello'\n-- OR\nSELECT to_tsquery('english', 'hello \u0026 worlds');  --\u003e 'hello' \u0026 'world'\n```\n\n - `plainto_tsquery(config, text) -\u003e tsquery`: Accepts a more generic search term. by default each word in the query is an \"\u0026\" operation\n```sql\nSELECT plainto_tsquery('english', 'hello world'); --\u003e 'hello' \u0026 'world'\n```\n - `websearch_to_tsquery(config, text) -\u003e tsquery`: This one is a bit more sophisticated and my favourite. It uses a google type syntax for searching.\n```sql\nSELECT websearch_to_tsquery('simple', '\"hello there\" -world');  --\u003e  'hello' \u003c-\u003e 'there' \u0026 !'world'\n```\n\n### Example usage\n```sql\nSELECT message FROM mock_data\nWHERE \n    to_tsvector('english', message) \n    @@ \n    websearch_to_tsquery('english', 'product killer -content')\nLIMIT 5; \n             message             \n---------------------------------\n productize killer architectures\n productize killer synergies\n(2 rows)\n```\n\n### Improving performance\nTo get some really good performance on your queries. Create a generated column with the tsvector data and then add a GIN index to that column. \n```sql\nALTER TABLE mock_data \n    ADD COLUMN ts_message_col tsvector \n    GENERATED ALWAYS AS (to_tsvector('english', message)) \n    STORED;\n\nCREATE INDEX idx_tsvector_message ON mock_data USING GIN(ts_message_col);\n```\n```sql\nSELECT message FROM mock_data\nWHERE \n    ts_message_col @@ websearch_to_tsquery('english', 'product or content')\nLIMIT 5; \n              message              \n-----------------------------------\n productize extensible initiatives\n target value-added content\n productize visionary content\n monetize proactive content\n synthesize cross-media content\n(5 rows)\n```\n\n## Trigrams (Fuzzier)\n\u003e [pg_trgm](https://www.postgresql.org/docs/current/pgtrgm.html) module required: `CREATE extension pg_trgm;`\n\nAs the name suggests, a trigram is a series of three consecutive characters from a string. For example, take the string *\"Hello world\".*\n```sql\nSELECT show_trgm('Hello world');\n                           show_trgm                           \n---------------------------------------------------------------\n {\"  h\",\"  w\",\" he\",\" wo\",ell,hel,\"ld \",llo,\"lo \",orl,rld,wor}\n```\nIn PostgreSQL, trigrams are used to generate a similarity score between two strings. \n[pg_trgm](https://www.postgresql.org/docs/current/pgtrgm.html) provides us with three functions for this:\n - `similarity(string, string)`: Similarity between the whole first and second string\n```sql\nSELECT similarity('hello', 'Helo world'); --\u003e 0.30769232\n-- OR\nSELECT 1 - ('hello' \u003c-\u003e 'Helo world');  --\u003e 0.307692289352417\n```\n - `word_similarity(string, string)`: The greatest similarity between the first string and any substring of the second string\n```sql\nSELECT word_similarity('hello', 'Helo world'); --\u003e 0.5714286\n-- OR\nSELECT 1 - ('hello' \u003c\u003c-\u003e 'Helo world'); --\u003e 0.5714285969734192\n```\n - `strict_word_similarity(string, string)`: The greatest similarity between the first string and any whole word in the second string\n```sql\nSELECT strict_word_similarity('hello', 'Helo world'); --\u003e 0.5714286\n-- OR\nSELECT 1 - ('hello' \u003c\u003c\u003c-\u003e 'Helo world'); --\u003e 0.5714285969734192\n```\n\n#### Or if you want the boolean results. \n```sql\nSELECT ('hello' % 'Helo world'); --\u003e similarity TRUE\nSELECT ('hello' \u003c% 'Helo world'); --\u003e word_similarity FALSE\nSELECT ('hello' \u003c\u003c% 'Helo world'); --\u003e strict_word_similarity TRUE\n```\nThe result of this depends on the following GUC parameters respectively\n- `pg_trgm.similarity_threshold` (default 0.3)\n- `pg_trgm.word_similarity_threshold` (default 0.6)\n- `pg_trgm.strict_word_similarity_threshold` (default 0.5)\n\n### Improving performance\nConveniently pg_trgm module provides GiST and GIN index operator classes that allow you to create an index over a text column. I haven't tested this out fully but apparently, the GIST index provides better performance. \n```sql\nCREATE INDEX trgm_idx_text_column ON test_table USING GIST (text_column gist_trgm_ops);\n-- OR\nCREATE INDEX trgm_idx_text_column ON test_table USING GIN (text_column gin_trgm_ops);\n```\n\n\n\n**See:** [PostgreSQL Docs: pg_trgm](https://www.postgresql.org/docs/current/pgtrgm.html)\n\n## Levenshtein distance (Fuzzier)\n\u003e[fuzzystrmatch](https://www.postgresql.org/docs/current/fuzzystrmatch.html) module required: `CREATE extension fuzzystrmatch;`\n\nLevenshtein distance is a measure of the similarity between two strings, measured in terms of the number of characters that need to be changed in order to turn one string into the other. \n\n```sql\nSELECT \n    first_name, \n    levenshtein(first_name, 'Bobby') AS difference FROM mock_data\nWHERE levenshtein(first_name, 'Bobby') \u003c 3\nORDER BY 2\nLIMIT 5;\n first_name | difference \n------------+------------\n Bobby      |          0\n Bobbi      |          1\n Bobbi      |          1\n Bibby      |          1\n Toby       |          2\n(5 rows)\n```\n### Performance Improvements\nOne of the issues with the Levenshtein method is that there is no way to index it as the index would need to know the input. However, there is something we can do. We can reduce the number of records it has to process by combining it with one of the more fuzzy options below.\n\n\n**See:** [PostgreSQL Docs: fuzzystrmatch](https://www.postgresql.org/docs/current/fuzzystrmatch.html#id-1.11.7.24.7)\n\n## Phonetic similarity (Very Fuzzy)\n\u003e[fuzzystrmatch](https://www.postgresql.org/docs/current/fuzzystrmatch.html) module required: `CREATE extension fuzzystrmatch;`\n\nFor me, I found these next couple of methods particularly interesting. Instead of measuring how similar words are to each other in terms of individual characters. We can actually compare them by how they sound when they are spoken.\nfuzzystrmatch provides three functions out of the box for this.\n - `soundex(string) -\u003e text`: converts a string to its Soundex code.\n```sql\nSELECT soundex('Anne'), soundex('Ann'), difference('Anne', 'Ann');\n soundex | soundex | difference \n---------+---------+------------\n A500    | A500    |          4\n(1 row)\n```\n - `metaphone(string, max_output_length) -\u003e text`: like Soundex, is based on the idea of constructing a representative code for an input string.\n```sql\nSELECT metaphone('brendan', 10), metaphone('brandon', 10);\n metaphone | metaphone \n-----------+-----------\n BRNTN     | BRNTN\n(1 row)\n```\n - `dmetaphone(string) -\u003e text`/`dmetaphone_alt(string) -\u003e text: computes two “sounds like” strings for a given input string — a “primary” and an “alternate”. In most cases, they are the same, but for non-English names especially they can be a bit different, depending on pronunciation. These functions compute the primary and alternate codes\n```sql\nSELECT dmetaphone_alt('brendan'), dmetaphone('Brandon');\n dmetaphone_alt | dmetaphone \n----------------+------------\n PRNT           | PRNT\n(1 row)\n```\n\n### performance improvements\nEach of these methods can be indexed using a normal function based index\n```sql\nCREATE INDEX idx_sdx_first_name ON mock_data (soundex(first_name));\n--OR\nCREATE INDEX idx_mtf_first_name ON mock_data (metaphone(first_name, 10));\n--OR\nCREATE INDEX idx_dmtf_first_name ON mock_data (dmetaphone(first_name));\n```\n I mentioned above that we can improve the Levenshtein method by combining it with one of these methods. Once you've indexed the column for one of the phonetics functions you can use that to reduce the dataset and use Levenshtein to finish the filtering\n```sql\nSELECT \n    first_name, \n    levenshtein(first_name, 'Bobby') AS difference FROM mock_data\nWHERE \n    soundex(first_name) = soundex('bobby')\nAND \n    levenshtein(first_name, 'Bobby') \u003c 3\nORDER BY 2\nLIMIT 5;\n first_name | difference \n------------+------------\n Bobby      |          0\n Bobbi      |          1\n Bobbi      |          1\n Bibby      |          1\n Bobbie     |          2\n(5 rows)\n\n``` \n\n**See:** [PostgreSQL Docs: fuzzystrmatch](https://www.postgresql.org/docs/current/fuzzystrmatch.html)"},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"postgresql_fuzzy_search"},"buildId":"huS74PAfV3qTdbv0QEjjy","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>