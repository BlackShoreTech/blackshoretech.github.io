{"pageProps":{"frontmatter":{"title":"PostgreSQL Fuzzy Text Search: Not so fuzzy to fuzziest","date":"June 7, 2022","author":"Brendan","excerpt":"So you have a bunch of data that comes from some human source (Free text form fields, reviews, blogs, classified ads, social media) and you want to do some analysis on it.","cover_image":"https://cdn.hashnode.com/res/hashnode/image/upload/v1654533752867/bDBd4SD-S.png?w=1600&h=840&fit=crop&crop=entropy&auto=compress,format&format=webp"},"slug":"postgresql_fuzzy_search","content":"So you have a bunch of data that comes from some human source (Free text form fields, reviews, blogs, classified ads, social media) and you want to do some analysis on it. but with people being the way they are, you're going to have some problems:\n1. A lot of words are commonly miss-spelled (definitely-> definitly etc).\n2. Regional differences. e.g American and British English (color/colour, analyse/analyze)\n3. Creative ways of spelling to add dramatic effect. (heyyy, whaaaaat!, noooo!)\n\nAll of this, plus more will affect your results and make it difficult to do any accurate analysis of the data (such as grouping similar topics together, etc). Luckily PostgreSQL comes packaged with a number of really useful tools that make life a lot easier for us. This is a complex topic and I'm only going to touch on the basics. But there is enough here to cover most basic and possibly some more complex use-cases.  \n\n## Simple Pattern matching (A little Fuzzy)\n#### LIKE\nUseful when you have a good idea of what the data and queries look like but it's difficult to create something generic enough to be useful in a general text dataset. With this, you only have two ways to match the text. `%` is used as a wildcard for 0 or more characters of any value, and `_` matches a single character of any value.\n\n - `LIKE`: use wildcards and character substitution, Case sensitive\n```sql\nSELECT 'Hello world' LIKE 'He__o %';  -- TRUE\n```\n - `ILIKE`: use wildcards and character substitution, Case insensitive\n```sql\nSELECT 'Hello world' ILIKE 'h_llo _%'; -- TRUE\n```\n - `NOT LIKE`/`NOT ILIKE`: inverse of LIKE or ILIKE\n```sql\nSELECT 'Hello world' NOT ILIKE '%_llo world%'; -- FALSE\n```\n\n#### Regex\nA little more advanced than the \"LIKE\" operator. With regex, you have a lot more control over the pattern matching. PostgreSQL comes with two standard ways to do this. \n - `[NOT] SIMILAR TO`: Uses a simpler SQL standard expression syntax which is kind of like a mix between the LIKE syntax and POSIX regular expressions. You can prepend `NOT` to negate the expression.\n```sql\nSELECT 'Hello world' SIMILAR TO 'H(e|a)l+o %'; --TRUE\n```\n - `~`/`!~`/`~*`/`!~*`: More powerful POSIX syntax that you may already be familiar with in other languages. `*` makes the expressions case-insensitive. `!` negates the expression\n```sql\nSELECT 'Hello world' ~ '^H(e|a)l{1,2}o [a-zA-Z]{5}$'; -- TRUE\n```\n\n### Improving performance\nIn certain scenarios it's possible to speed up your queries using a special operator class for a BTREE index. `text_pattern_ops` and `varchar_pattern_ops` allow you to index a text or varchar field. However, this is only effective if your queries are *left-anchored* (No leading wildcard) e.g `WHERE text_fields LIKE 'hell_ %'`\n \n\nI've only covered the basics of what you can do with regex and PostgreSQL, so I suggest looking at the docs below to learn more. \n\n**See:** [PostgreSQL Docs -> pattern-matchine](https://www.postgresql.org/docs/current/functions-matching.html)\n\n## Text Search Vectors (Fuzzy(ish))\nThis is probably the most efficient option for performing a full-text search. It works by removing all the stop words (it, the, as, by, ...) and duplicates from your text and reducing each word into its main component. For example *quick*, *quickly* just becomes *quick* and *product*, *production*, *products* becomes *product*. This provides a small bit of fuzziness to results as the query does not need the exact word. One caveat with tsvectors is that to use it effectively you need to know the language of the text. you can use the `'simple'` config option but you lose a lot of the efficiencies. \n\nThe first function you'll need is `to_tsvector(config, text)`. The result of this is a special datatype `tsvector` that contains each component along with its index in the original text.\n\n```sql\nselect to_tsvector('english', 'the quick brown fox ran quickly to the other foxes');\n               to_tsvector                \n-------------------------------------\n 'brown':3 'fox':4,10 'quick':2,6 'ran':5\n(1 row)\n```\n\nThe second thing you need is the query generator, which comes in a few different flavors\n - `to_tsquery(config, text) -> tsquery`: Creates a basic query from a single token or multiple if\nused with boolean operators.\n```sql\nSELECT to_tsquery('english', 'hello');  --> 'hello'\n-- OR\nSELECT to_tsquery('english', 'hello & worlds');  --> 'hello' & 'world'\n```\n\n - `plainto_tsquery(config, text) -> tsquery`: Accepts a more generic search term. by default each word in the query is an \"&\" operation\n```sql\nSELECT plainto_tsquery('english', 'hello world'); --> 'hello' & 'world'\n```\n - `websearch_to_tsquery(config, text) -> tsquery`: This one is a bit more sophisticated and my favourite. It uses a google type syntax for searching.\n```sql\nSELECT websearch_to_tsquery('simple', '\"hello there\" -world');  -->  'hello' <-> 'there' & !'world'\n```\n\n### Example usage\n```sql\nSELECT message FROM mock_data\nWHERE \n    to_tsvector('english', message) \n    @@ \n    websearch_to_tsquery('english', 'product killer -content')\nLIMIT 5; \n             message             \n---------------------------------\n productize killer architectures\n productize killer synergies\n(2 rows)\n```\n\n### Improving performance\nTo get some really good performance on your queries. Create a generated column with the tsvector data and then add a GIN index to that column. \n```sql\nALTER TABLE mock_data \n    ADD COLUMN ts_message_col tsvector \n    GENERATED ALWAYS AS (to_tsvector('english', message)) \n    STORED;\n\nCREATE INDEX idx_tsvector_message ON mock_data USING GIN(ts_message_col);\n```\n```sql\nSELECT message FROM mock_data\nWHERE \n    ts_message_col @@ websearch_to_tsquery('english', 'product or content')\nLIMIT 5; \n              message              \n-----------------------------------\n productize extensible initiatives\n target value-added content\n productize visionary content\n monetize proactive content\n synthesize cross-media content\n(5 rows)\n```\n\n## Trigrams (Fuzzier)\n> [pg_trgm](https://www.postgresql.org/docs/current/pgtrgm.html) module required: `CREATE extension pg_trgm;`\n\nAs the name suggests, a trigram is a series of three consecutive characters from a string. For example, take the string *\"Hello world\".*\n```sql\nSELECT show_trgm('Hello world');\n                           show_trgm                           \n---------------------------------------------------------------\n {\"  h\",\"  w\",\" he\",\" wo\",ell,hel,\"ld \",llo,\"lo \",orl,rld,wor}\n```\nIn PostgreSQL, trigrams are used to generate a similarity score between two strings. \n[pg_trgm](https://www.postgresql.org/docs/current/pgtrgm.html) provides us with three functions for this:\n - `similarity(string, string)`: Similarity between the whole first and second string\n```sql\nSELECT similarity('hello', 'Helo world'); --> 0.30769232\n-- OR\nSELECT 1 - ('hello' <-> 'Helo world');  --> 0.307692289352417\n```\n - `word_similarity(string, string)`: The greatest similarity between the first string and any substring of the second string\n```sql\nSELECT word_similarity('hello', 'Helo world'); --> 0.5714286\n-- OR\nSELECT 1 - ('hello' <<-> 'Helo world'); --> 0.5714285969734192\n```\n - `strict_word_similarity(string, string)`: The greatest similarity between the first string and any whole word in the second string\n```sql\nSELECT strict_word_similarity('hello', 'Helo world'); --> 0.5714286\n-- OR\nSELECT 1 - ('hello' <<<-> 'Helo world'); --> 0.5714285969734192\n```\n\n#### Or if you want the boolean results. \n```sql\nSELECT ('hello' % 'Helo world'); --> similarity TRUE\nSELECT ('hello' <% 'Helo world'); --> word_similarity FALSE\nSELECT ('hello' <<% 'Helo world'); --> strict_word_similarity TRUE\n```\nThe result of this depends on the following GUC parameters respectively\n- `pg_trgm.similarity_threshold` (default 0.3)\n- `pg_trgm.word_similarity_threshold` (default 0.6)\n- `pg_trgm.strict_word_similarity_threshold` (default 0.5)\n\n### Improving performance\nConveniently pg_trgm module provides GiST and GIN index operator classes that allow you to create an index over a text column. I haven't tested this out fully but apparently, the GIST index provides better performance. \n```sql\nCREATE INDEX trgm_idx_text_column ON test_table USING GIST (text_column gist_trgm_ops);\n-- OR\nCREATE INDEX trgm_idx_text_column ON test_table USING GIN (text_column gin_trgm_ops);\n```\n\n\n\n**See:** [PostgreSQL Docs: pg_trgm](https://www.postgresql.org/docs/current/pgtrgm.html)\n\n## Levenshtein distance (Fuzzier)\n>[fuzzystrmatch](https://www.postgresql.org/docs/current/fuzzystrmatch.html) module required: `CREATE extension fuzzystrmatch;`\n\nLevenshtein distance is a measure of the similarity between two strings, measured in terms of the number of characters that need to be changed in order to turn one string into the other. \n\n```sql\nSELECT \n    first_name, \n    levenshtein(first_name, 'Bobby') AS difference FROM mock_data\nWHERE levenshtein(first_name, 'Bobby') < 3\nORDER BY 2\nLIMIT 5;\n first_name | difference \n------------+------------\n Bobby      |          0\n Bobbi      |          1\n Bobbi      |          1\n Bibby      |          1\n Toby       |          2\n(5 rows)\n```\n### Performance Improvements\nOne of the issues with the Levenshtein method is that there is no way to index it as the index would need to know the input. However, there is something we can do. We can reduce the number of records it has to process by combining it with one of the more fuzzy options below.\n\n\n**See:** [PostgreSQL Docs: fuzzystrmatch](https://www.postgresql.org/docs/current/fuzzystrmatch.html#id-1.11.7.24.7)\n\n## Phonetic similarity (Very Fuzzy)\n>[fuzzystrmatch](https://www.postgresql.org/docs/current/fuzzystrmatch.html) module required: `CREATE extension fuzzystrmatch;`\n\nFor me, I found these next couple of methods particularly interesting. Instead of measuring how similar words are to each other in terms of individual characters. We can actually compare them by how they sound when they are spoken.\nfuzzystrmatch provides three functions out of the box for this.\n - `soundex(string) -> text`: converts a string to its Soundex code.\n```sql\nSELECT soundex('Anne'), soundex('Ann'), difference('Anne', 'Ann');\n soundex | soundex | difference \n---------+---------+------------\n A500    | A500    |          4\n(1 row)\n```\n - `metaphone(string, max_output_length) -> text`: like Soundex, is based on the idea of constructing a representative code for an input string.\n```sql\nSELECT metaphone('brendan', 10), metaphone('brandon', 10);\n metaphone | metaphone \n-----------+-----------\n BRNTN     | BRNTN\n(1 row)\n```\n - `dmetaphone(string) -> text`/`dmetaphone_alt(string) -> text: computes two “sounds like” strings for a given input string — a “primary” and an “alternate”. In most cases, they are the same, but for non-English names especially they can be a bit different, depending on pronunciation. These functions compute the primary and alternate codes\n```sql\nSELECT dmetaphone_alt('brendan'), dmetaphone('Brandon');\n dmetaphone_alt | dmetaphone \n----------------+------------\n PRNT           | PRNT\n(1 row)\n```\n\n### performance improvements\nEach of these methods can be indexed using a normal function based index\n```sql\nCREATE INDEX idx_sdx_first_name ON mock_data (soundex(first_name));\n--OR\nCREATE INDEX idx_mtf_first_name ON mock_data (metaphone(first_name, 10));\n--OR\nCREATE INDEX idx_dmtf_first_name ON mock_data (dmetaphone(first_name));\n```\n I mentioned above that we can improve the Levenshtein method by combining it with one of these methods. Once you've indexed the column for one of the phonetics functions you can use that to reduce the dataset and use Levenshtein to finish the filtering\n```sql\nSELECT \n    first_name, \n    levenshtein(first_name, 'Bobby') AS difference FROM mock_data\nWHERE \n    soundex(first_name) = soundex('bobby')\nAND \n    levenshtein(first_name, 'Bobby') < 3\nORDER BY 2\nLIMIT 5;\n first_name | difference \n------------+------------\n Bobby      |          0\n Bobbi      |          1\n Bobbi      |          1\n Bibby      |          1\n Bobbie     |          2\n(5 rows)\n\n``` \n\n**See:** [PostgreSQL Docs: fuzzystrmatch](https://www.postgresql.org/docs/current/fuzzystrmatch.html)"},"__N_SSG":true}